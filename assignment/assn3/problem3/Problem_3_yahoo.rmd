---
title: "Stock Price from Yahoo Extraction"
author: "Andrea Wang"
date: "2025-09-30"
output: html_document
---
```{r setup, include=FALSE}
# install.packages(c("tidyquant","BatchGetSymbols","tidyverse","rvest","httr","dplyr","stringr","readr"))
library(tidyquant)
library(BatchGetSymbols)
library(tidyverse)
library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(readr)
library(data.table)
library(furrr)
library(future)
library(arrow)
library(fs)
```

```{r sp500_member}
####### Get current S&P500 membership with sectors #######
# Returns a tibble with columns: symbol, company, sector
get_sp500_with_sectors <- function() {
  url <- "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
  pg  <- GET(url, user_agent("Mozilla/5.0"))      
  tab <- html_table(read_html(pg) |> html_element("table.wikitable"), fill = TRUE)

  tibble(
    symbol  = str_replace(tab$Symbol, "\\.", "-"),  # BRK.B -> BRK-B for Yahoo
    company = tab$Security,
    sector  = tab$`GICS Sector`
  )
}

members <- get_sp500_with_sectors()
head(members)
write_csv(members,   "sp500_meta.csv") 
```


```{r get_price_function}
####### Download **raw prices** (adjusted by default) for any tickers #######
# Returns a wide tibble: first column \code{date}, remaining columns one per ticker.

# @param tickers Character vector of ticker symbols.
# @param start Start date (Date). Default \code{as.Date("2019-01-01")}.
# @param end End date (Date). Default \code{Sys.Date()}.
# @param freq One of \code{"daily"}, \code{"weekly"}, \code{"monthly"}.
# @param price_col Which price column to return:
#   \itemize{
#     \item \code{"price.adjusted"} (default): split/dividend-adjusted close (recommended).
#     \item \code{"price.close"}: unadjusted closing price.
#   }
# @param cache_dir Directory path for on-disk cache (created if missing).
#   Set to \code{NULL} to disable caching.
# @param join How to align dates across tickers in the wide result:
#   \itemize{
#     \item \code{"outer"} (default): keep all dates; missing values left as NA.
#     \item \code{"inner"}: keep only dates present for **all** tickers (drop rows with any NA).
#'   }
#'
get_prices_yahoo <- function(tickers,
                             start = as.Date("2019-01-01"),
                             end   = Sys.Date(),
                             freq  = c("daily","weekly","monthly"),
                             price_col = c("price.adjusted","price.close"),
                             cache_dir  = file.path(getwd(), "bg_cache")) {
  freq      <- match.arg(freq)
  price_col <- match.arg(price_col)

  bg <- BatchGetSymbols(
    tickers      = unique(tickers),
    first.date   = start,
    last.date    = end,
    freq.data    = freq,
    cache.folder = cache_dir,      # on-disk cache to avoid re-downloads
    do.cache = TRUE    # Use caching for faster future downloads
  )

  prices_long <- bg$df.tickers |>
    select(ref.date, ticker, all_of(price_col)) |>
    rename(date = ref.date, price = !!price_col) |>
    distinct(date, ticker, .keep_all = TRUE) |>
    arrange(date, ticker)

  prices_wide <- prices_long |>
    pivot_wider(names_from = ticker, values_from = price) |>
    arrange(date)

  prices_wide
}
```


```{r class example}
etfs <- c(
  EWJ="Japan", EWZ="Brazil", FXI="China", EWY="South Korea",
  EWT="Taiwan", EWH="Hong Kong", EWC="Canada", EWG="Germany",
  EWU="United Kingdom", EWA="Australia", EWW="Mexico", EWL="Switzerland",
  EWP="Spain", EWQ="France", EIDO="Indonesia", ERUS="Russia",
  EWS="Singapore", EWM="Malaysia", EZA="South Africa", THD="Thailand",
  ECH="Chile", EWI="Italy", TUR="Turkey", EPOL="Poland",
  EPHE="Philippines", EWD="Sweden", EWN="Netherlands", EPU="Peru",
  ENZL="New Zealand", EIS="Israel", EWO="Austria", EIRL="Ireland", EWK="Belgium"
)
tickers_manual <- names(etfs)

price_manual_wk <- get_prices_yahoo(tickers_manual, start = as.Date("2017-03-20"), end = as.Date("2022-03-21"),
                                 freq = "weekly", price_col = "price.close")
head(price_manual_wk)
write_csv(price_manual_wk,   "weekly_stock.csv")
```

```{r}
####### Download a broad universe of tickers (e.g., stock index and stock exchange #######

symbol_stockindex <- tryCatch({
  symbol <- c()
  for (q in tq_index_options()){
    symbol <- c(symbol, tq_index(q)$symbol)
  }
  unique(symbol)
}, error = function(e) character())

symbol_exchange <- tryCatch({
  symbol <- c()
  for (q in tq_exchange_options()){
    symbol <- c(symbol, tq_exchange(q)$symbol)
  }
  unique(symbol)
}, error = function(e) character())
tickers_full <- unique(c(symbol_stockindex, symbol_exchange))
length(tickers_full)  # how many tickers?
```

```{r}
start_date <- as.Date("2020-03-01")  # change your COVID cutoff if needed
price_manual_wk <- get_prices_yahoo(symbol_stockindex, start = start_date, end = Sys.Date(), freq = "weekly", price_col = "price.close")
price_exchange_wk <- get_prices_yahoo(symbol_exchange, start = start_date, end = Sys.Date(), freq = "weekly", price_col = "price.close")
```

```{r}
symbol_index_wk <- colnames(price_manual_wk)[-1]
symbol_exchange_wk <- colnames(price_exchange_wk)[-1]
```

```{r}
start_date <- as.Date("2020-03-01")  # change your COVID cutoff if needed
price_manual_daily <- get_prices_yahoo(symbol_stockindex, start = start_date, end = Sys.Date(), freq = "daily", price_col = "price.close")
price_exchange_daily <- get_prices_yahoo(symbol_exchange, start = start_date, end = Sys.Date(), freq = "daily", price_col = "price.close")
```
sym

```{r}
tickers <- tickers_full
# If you already have a big vector, just assign tickers <- your_vector

start_date <- as.Date("2020-03-01")  # change your COVID cutoff if needed

end_date   <- Sys.Date()

workers        <- max(1, parallel::detectCores() - 1)
chunk_size     <- 300
max_retries    <- 3
pause_between  <- 0.5

out_dir   <- "yahoo_bulk_out"
cache_dir <- file.path(out_dir, "cache")


fs::dir_create(out_dir, recurse = TRUE)
fs::dir_create(cache_dir, recurse = TRUE)

# ----- chunker -----
make_chunks <- function(x, chunk_size) {
  n <- length(x); starts <- seq(1, n, by = chunk_size)
  lapply(starts, function(s) x[s:min(s + chunk_size - 1, n)])
}

# EXAMPLE: provide your 7k symbols vector here
# tickers <- readr::read_lines("tickers_master.txt")
chunks  <- make_chunks(tickers, chunk_size)

message(sprintf("tickers=%d  chunk_size=%d  n_chunks=%d",
                length(tickers), chunk_size, length(chunks)))

# ----- worker-safe downloader (all calls are pkg-qualified) -----
download_chunk <- function(chunk_syms, idx,
                           start_date, end_date, out_dir, cache_dir,
                           max_retries, pause_between) {
  chunk_tag  <- sprintf("chunk_%04d", idx)
  chunk_file <- file.path(out_dir, paste0(chunk_tag, ".rds"))
  fail_file  <- file.path(out_dir, paste0(chunk_tag, "_fails.csv"))

  if (fs::file_exists(chunk_file)) {
    message(chunk_tag, " exists â€” skipping")
    return(list(ok = TRUE, file = chunk_file))
  }

  attempt <- 1
  last_err <- NULL
  repeat {
    message(chunk_tag, " attempt ", attempt, " (", length(chunk_syms), " tickers)")
    tryres <- try({
      res <- BatchGetSymbols::BatchGetSymbols(
        tickers      = chunk_syms,
        first.date   = start_date,
        last.date    = end_date,
        cache.folder = cache_dir,
        do.parallel  = FALSE,     # outer parallel only
        max.fail.no  = 100
      )

      dt_prices  <- data.table::as.data.table(res$df.tickers)
      dt_control <- data.table::as.data.table(res$df.control)

      if (nrow(dt_control)) {
        fails <- dt_control[download.status != "OK"]
        if (nrow(fails)) data.table::fwrite(fails, fail_file)
      }

      saveRDS(dt_prices, chunk_file)
      message(chunk_tag, " saved with ", nrow(dt_prices), " rows")
      list(ok = TRUE, file = chunk_file)
    }, silent = TRUE)

    if (inherits(tryres, "try-error")) {
      last_err <- tryres
      if (attempt >= max_retries) {
        message(chunk_tag, " failed after ", max_retries, " attempts")
        readr::write_lines(as.character(last_err), fail_file)
        return(list(ok = FALSE, file = NULL))
      }
      Sys.sleep(pause_between); attempt <- attempt + 1
    } else {
      return(tryres)
    }
  }
}

# ----- run in parallel across chunks -----
plan(multisession, workers = workers)
on.exit(plan(sequential), add = TRUE)

results <- furrr::future_imap(
  chunks,
  ~ download_chunk(.x, .y, start_date, end_date, out_dir, cache_dir, max_retries, pause_between),
  .progress = TRUE
)

# ----- collect outputs (no %||%) -----
rds_files <- vapply(results, function(x) if (is.null(x$file)) NA_character_ else x$file, character(1))
rds_files <- rds_files[fs::file_exists(rds_files)]

if (!length(rds_files)) stop("No chunk files produced. Check *_fails.csv in ", out_dir)

# ----- combine & write parquet once -----
all_dt <- data.table::rbindlist(lapply(rds_files, readRDS), use.names = TRUE, fill = TRUE)
parquet_file <- file.path(out_dir, "prices_since_covid.parquet")
arrow::write_parquet(all_dt, parquet_file)

message("Done. Rows: ", nrow(all_dt),
        " | Tickers: ", length(unique(all_dt$ticker)),
        " | Parquet: ", parquet_file)

```

